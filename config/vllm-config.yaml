# vLLM Server Configuration for Fara-7B on DGX Spark
# Optimized for NVIDIA Blackwell GB10 architecture with 128GB unified memory

#===============================================================================
# Server Configuration
#===============================================================================
server:
  host: "0.0.0.0"
  port: 5000
  
#===============================================================================
# Model Configuration
#===============================================================================
model:
  # Microsoft Fara-7B - Vision-language model for computer use
  name: "microsoft/Fara-7B"
  
  # Precision settings
  # - "auto": Automatically selects best precision (FP16 on Blackwell)
  # - "float16": Force FP16 precision
  # - "bfloat16": Force BF16 precision (if supported)
  # - "float8": FP8 quantization (reduced memory, slight accuracy loss)
  dtype: "auto"
  
  # Maximum sequence length
  # Fara-7B supports up to 16384 tokens context
  max_model_len: 16384
  
  # Trust remote code for custom model architectures
  trust_remote_code: true
  
  # Quantization (optional)
  # Uncomment for memory-constrained scenarios
  # quantization: "awq"  # or "gptq", "fp8"

#===============================================================================
# Blackwell GB10 GPU Optimizations
#===============================================================================
gpu:
  # GPU memory utilization fraction
  # DGX Spark's 128GB unified memory allows higher utilization
  # 0.85 = 85% GPU memory (~108GB for Fara-7B)
  memory_utilization: 0.85
  
  # Enable CUDA graphs for better performance
  # Set to true for production, false for debugging
  enforce_eager: false
  
  # Tensor parallelism (single GPU on Spark)
  tensor_parallel_size: 1

#===============================================================================
# Inference Settings
#===============================================================================
inference:
  # Maximum concurrent sequences
  # Higher values = better throughput, more memory
  max_num_seqs: 256
  
  # Maximum batched tokens
  max_num_batched_tokens: 32768
  
  # Block size for KV cache
  block_size: 16
  
  # Swap space for CPU offloading (GB)
  # Useful when GPU memory is constrained
  swap_space: 4

#===============================================================================
# Generation Defaults
#===============================================================================
generation:
  # Default temperature for sampling
  temperature: 0.0  # Deterministic for agentic tasks
  
  # Top-p sampling
  top_p: 1.0
  
  # Maximum new tokens to generate
  max_tokens: 2048
  
  # Stop sequences
  stop_sequences:
    - "<|endoftext|>"
    - "<|im_end|>"

#===============================================================================
# API Configuration
#===============================================================================
api:
  # OpenAI-compatible API
  api_key: "local-vllm"
  
  # Chat completions endpoint
  chat_template: null  # Use model's default
  
  # Enable streaming
  streaming: true

#===============================================================================
# Logging Configuration
#===============================================================================
logging:
  level: "INFO"
  log_requests: true
  log_responses: false
  
#===============================================================================
# Health Checks
#===============================================================================
health:
  endpoint: "/health"
  ready_endpoint: "/ready"
  
#===============================================================================
# Performance Tuning Notes for DGX Spark
#===============================================================================
# 
# Memory Management:
# - Clear cache before starting: sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
# - Monitor with: nvidia-smi dmon -s mu
#
# Precision Guidelines:
# - FP16/auto: Best accuracy, ~14GB model + KV cache
# - FP8: Good accuracy, ~7GB model + KV cache
# - INT4 (AWQ/GPTQ): Lower accuracy, ~3.5GB model
#
# Throughput Optimization:
# - Increase max_num_seqs for batch processing
# - Enable CUDA graphs (enforce_eager: false)
# - Use larger block_size for longer sequences
#
# Latency Optimization:
# - Reduce max_num_seqs for lower latency
# - Use smaller max_model_len if sequences are short
# - Enable eager mode for debugging
